# ‚úàÔ∏è ETL con Apache Airflow, SQLite y Streamlit

Proyecto educativo para aprender a construir pipelines ETL usando Apache Airflow, almacenar datos en SQLite y visualizarlos en un dashboard interactivo con Streamlit.

En este caso trabajaremos con los datos de. E-commerce Olist

---

Objetivo: Completar las tareas faltantes en el DAG y desarrollar los m√≥dulos correspondientes a la etapa de limpieza (Silver) y transformaci√≥n (Gold) para consolidar un pipeline ETL en Airflow.

---

üß© Parte 1: Completa el DAG.

En esta primera parte, debes completar la definici√≥n del DAG importando las funciones cleaning_data y transform_data desde sus respectivos scripts. Luego, agrega dos tareas al flujo: una para la limpieza de datos correspondiente al stage Silver (task_id='cleaning_data') y otra para la transformaci√≥n de datos en el stage Gold (task_id='transform_data'), asignando en cada caso la funci√≥n correspondiente como python_callable. A continuaci√≥n, define la secuencia completa del pipeline utilizando operadores de dependencia para que las tareas se ejecuten en el orden correcto: extract >> load >> [...]. Finalmente, configura una periodicidad de ejecuci√≥n adecuada; si est√°s en modo de prueba, puedes usar '@once' para que el DAG se ejecute una sola vez al activarlo.

---

üß™ Parte 2: Crea los m√≥dulos cleaning.py y transform.py.

En la segunda parte, tu objetivo es construir los m√≥dulos cleaning.py y transform.py que se encargan de las etapas Silver y Gold del pipeline. 

En cleaning.py, debes conectarte a la base de datos ecommerce.db ubicada en /opt/airflow/dags/data, y crear tres tablas silver_* como copias exactas de las tablas bronze_* utilizando sentencias SQL del tipo CREATE TABLE AS SELECT. Aunque normalmente este paso incluir√≠a limpieza de datos (duplicados, nulos, formatos), en este caso solo simular√°s la limpieza realizando una copia espejo. No olvides cerrar la conexi√≥n y utilizar print() para mostrar mensajes informativos sobre el estado del proceso. 

En transform.py, trabajar√°s sobre las tablas Silver y ejecutar√°s dos consultas SQL que ya formulaste previamente: una para calcular el top 10 de estados con mayor ingreso (gold_top_states), y otra para comparar los tiempos de entrega reales vs. estimados por mes y a√±o (gold_delivery_comparison). Ambas consultas deben ser definidas como cadenas de texto y ejecutadas con cursor.executescript(...). Finalmente, confirma la ejecuci√≥n exitosa con mensajes por consola y cierra la conexi√≥n correctamente.

---

üé¨ Parte 3: Presentaci√≥n en video + An√°lisis del dashboard

En esta etapa final, deber√°s realizar una presentaci√≥n en video donde expliques el flujo completo de tu pipeline ETL construido con Airflow. En el video, describe brevemente qu√© hace cada etapa del c√≥digo (extract, load, clean, transform) y c√≥mo se conectan entre s√≠ dentro del DAG. Adem√°s, incluye una navegaci√≥n por el dashboard desarrollado en Streamlit. Explica c√≥mo se visualizan los resultados generados a partir de las tablas gold_top_states y gold_delivery_comparison.

A continuaci√≥n, realiza un an√°lisis cr√≠tico del dashboard. Responde: ¬øQu√© datos relevantes se pueden obtener? ¬øQu√© patrones o hallazgos llaman la atenci√≥n? ¬øQu√© decisiones podr√≠a tomar la compa√±√≠a a partir de esta informaci√≥n? Por ejemplo, ¬øhay estados que claramente deber√≠an recibir m√°s atenci√≥n? ¬øExisten diferencias estacionales en los tiempos de entrega que afecten la experiencia del cliente?

Por √∫ltimo, prop√≥n al menos dos recomendaciones de negocio basadas en los datos analizados, y una posible mejora al pipeline o al dashboard que podr√≠as implementar en una futura iteraci√≥n.

## üìÑ Licencia
Este proyecto est√° bajo la licencia MIT.
Puedes usarlo, modificarlo y compartirlo libremente con atribuci√≥n.

## ‚ú® Autor
Desarrollado por @SharonCamacho para ense√±ar y aprender pipelines y visualizaci√≥n.

